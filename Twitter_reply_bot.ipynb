{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Reply Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a twitter reply bot that replies to comments automatically with relevant responses when taggen with @GivaRep.\n",
    "\n",
    "# Basically it find the mention(tag) then readts the pjarent tweet it is beign mentioned on\n",
    "\n",
    "# The response is posted and the tweet is logged \n",
    "\n",
    "# The notebook will focus on point number 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#At first we initialize our llm\n",
    "\n",
    "llm = ChatVertexAI(temperature=0.2, max_output_tokens=200,)\n",
    "llm.predict_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that will generate response based on a tweet\n",
    "\n",
    "def generate_response(llm, mentioned_parent_tweet_text):\n",
    "    # It would be niec to bring in ingormation about the links, pictures, etc.\n",
    "    system_template = \"\"\"\n",
    "        You are a wity and ironic helper bot. Your goal is to find a quote which is quoted by some popular entity whose meaning is ironic in response to a piece of text from the user.\n",
    "\n",
    "        % RESPONSE TONE:\n",
    "\n",
    "        * Your prediction should be given in an active voice and be opinionated\n",
    "        * Your tone should be serious w/ a hint of wit and sarcasm\n",
    "\n",
    "        % RESPONSE FORMAT:\n",
    "\n",
    "        * Respond in under 200 characters\n",
    "        * Respond in two or less short sentences\n",
    "        * Do not respond with emojis\n",
    "\n",
    "        % RESPONSE CONTENT:\n",
    "\n",
    "        * Include specific examples of old tech if they are relevant\n",
    "        * If you dont have an answer, say, \"Sorry, my balls aint workin\"\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(template=system_template)\n",
    "\n",
    "    human_template = \"{text}\"\n",
    "\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(template=human_template)\n",
    "\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])\n",
    "\n",
    "    # get a chat completion from the formatted messages\n",
    "\n",
    "    final_prompt = chat_prompt.format_prompt(text = mentioned_parent_tweet_text)\n",
    "    #print(final_prompt)\n",
    "    response = llm(final_prompt.to_messages()).content\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I'm not saying we're going to start a war, but I'm not saying we won't either.\" - Donald Trump\n"
     ]
    }
   ],
   "source": [
    "tweet = \"\"\"Talks are ongoing over the revival of the 2015 Iran nuclear deal.  The Iranians say they are prepared to continue negotiations with the U.S.\"\"\"\n",
    "\n",
    "response = generate_response(llm, tweet)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
